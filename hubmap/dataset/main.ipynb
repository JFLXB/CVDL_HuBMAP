{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo\n",
    "\n",
    "Toy notebook to demonstate usage of Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.patches as mpatches\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from datasets import BaseDataset, ExpertDataset, DatasetFromSubset\n",
    "import transforms as tran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform wie RandomXYZ f端hren zu Data Augmentation,\n",
    "# weil das modell jeden epoch ein anderes bild kriegt, sollte aber dann nichtmehr f端rs\n",
    "# test set verwendet  werden.\n",
    "\n",
    "transforms_augment = tran.Compose(\n",
    "    [\n",
    "        tran.ToTensor(),\n",
    "        tran.Resize((256, 256)),\n",
    "        tran.RandomHorizontalFlip(),\n",
    "        tran.RandomVerticalFlip(),\n",
    "        tran.RandomGamma(),  # aufpassen, die default values sind nicht gut\n",
    "        tran.RandomHueSaturationValue(),  # aufpassen, die default values sind nicht gut\n",
    "        tran.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transforms_val = tran.Compose(\n",
    "    [\n",
    "        tran.ToTensor(),\n",
    "        tran.Resize((256, 256)),\n",
    "        tran.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = ExpertDataset(\"../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hier auch wieder sehr wichtig, das Dataset mit random transformations\n",
    "# sollte nat端rlich nicht zum evaluieren verwendet werden.\n",
    "train_ratio = 0.8\n",
    "train_size = int(train_ratio * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset_sub, test_dataset_sub = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "\n",
    "# TODO: Das ist horrible und man kann die plot methoden von dem eigentlichen Dataset nicht callen\n",
    "# muss ich mir was besseres 端berlegen\n",
    "train_dataset = DatasetFromSubset(train_dataset_sub, transform=transforms_augment)\n",
    "test_dataset = DatasetFromSubset(test_dataset_sub, transform=transforms_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels, out_channels, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.skip = nn.Identity()\n",
    "        if in_channels != out_channels:\n",
    "            self.skip = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.skip(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.init_conv = nn.Conv2d(in_channels, 16, kernel_size=3, padding=1, stride=1)\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            ResidualBlock(16, 16),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            ResidualBlock(16, 32),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            ResidualBlock(32, 64),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.bottleneck = ResidualBlock(64, 64)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2),\n",
    "            ResidualBlock(128, 64),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2),\n",
    "            ResidualBlock(64, 32),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2),\n",
    "            ResidualBlock(32, 16),\n",
    "        )\n",
    "\n",
    "        self.out_conv = nn.Conv2d(16, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.init_conv(x)\n",
    "\n",
    "        enc1 = self.encoder[0](x)\n",
    "        enc2 = self.encoder[2](self.encoder[1](enc1))\n",
    "        enc3 = self.encoder[4](self.encoder[3](enc2))\n",
    "\n",
    "        bottleneck = self.bottleneck(self.encoder[5](enc3))\n",
    "\n",
    "        dec1 = self.decoder[1](torch.cat([enc3, self.decoder[0](bottleneck)], dim=1))\n",
    "        dec2 = self.decoder[3](torch.cat([enc2, self.decoder[2](dec1)], dim=1))\n",
    "        dec3 = self.decoder[5](torch.cat([enc1, self.decoder[4](dec2)], dim=1))\n",
    "\n",
    "        x = self.out_conv(dec3)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(in_channels=3, out_channels=3)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\transforms\\functional.py:152: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:212.)\n",
      "  img = torch.from_numpy(pic.transpose((2, 0, 1))).contiguous()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.1314595350552173, Train Acc: 4.572991164572515, Test Loss: 0.14730903059244155, Test Acc: 2.0212689568014706\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "train_loss = []\n",
    "train_accuracy = []\n",
    "test_loss = []\n",
    "test_accuracy = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        labels = torch.argmax(labels, dim=1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += torch.numel(labels)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    train_accuracy.append(100 * correct / total)\n",
    "    train_loss.append(running_loss / i)\n",
    "\n",
    "    # Testing\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        labels = torch.argmax(labels, dim=1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += torch.numel(labels)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    test_accuracy.append(100 * correct / total)\n",
    "    test_loss.append(running_loss / i)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}, Train Loss: {train_loss[-1]}, Train Acc: {train_accuracy[-1]}, Test Loss: {test_loss[-1]}, Test Acc: {test_accuracy[-1]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
