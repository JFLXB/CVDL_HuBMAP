{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Pipeline For the FCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision.transforms as TV\n",
    "\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from hubmap.dataset import TrainDataset, ValDataset\n",
    "from hubmap.data import DATA_DIR\n",
    "from checkpoints import CHECKPOINT_DIR\n",
    "\n",
    "from hubmap.dataset import transforms as T\n",
    "from hubmap.losses import DiceBCELoss\n",
    "from hubmap.losses import MultiOutputBCELoss\n",
    "from hubmap.losses import ChannelWeightedDiceBCELoss\n",
    "from hubmap.metrics import Jac\n",
    "from hubmap.training import LRScheduler\n",
    "from hubmap.training import EarlyStopping\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the data that we need for the experiment. This includes the training data, the validation (test) data that we will use for training.\n",
    "\n",
    "For this, depending on the experiments we use different transformations on the data. The following transformations are a minimal example. Furhter transformations should be included for more sophisticated experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DIM = 224\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the `mean` and `std` of the `train` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(DATA_DIR / \"train_dataset_mean.pt\") and os.path.exists(DATA_DIR / \"train_dataset_std.pt\"):\n",
    "    train_mean = torch.load(DATA_DIR / \"train_dataset_mean.pt\")\n",
    "    train_std = torch.load(DATA_DIR / \"train_dataset_std.pt\")\n",
    "else:\n",
    "    train_calc = TrainDataset(DATA_DIR, transform=T.Compose([T.ToTensor(), T.Grayscale()]))\n",
    "    data = [train_calc[i] for i in range(len(train_calc))]\n",
    "    images = list(zip(*data))[0]\n",
    "    images = torch.stack(images)\n",
    "    train_mean = torch.mean(images, dim=(0, 2, 3))\n",
    "    train_std = torch.std(images, dim=(0, 2, 3))\n",
    "    torch.save(train_mean, DATA_DIR / \"train_dataset_mean.pt\")\n",
    "    torch.save(train_std, DATA_DIR / \"train_dataset_std.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean, train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(DATA_DIR / \"class_weights.pt\"):\n",
    "    weights = torch.load(Path(DATA_DIR / \"class_weights.pt\"))\n",
    "else:\n",
    "    # calcualte_class_imbalance\n",
    "    train_calc = TrainDataset(DATA_DIR, transform=T.ToTensor(), with_background=True)\n",
    "    targets = [train_calc[i][1] for i in range(len(train_calc))]\n",
    "    # targets = list(zip(*data))[1]\n",
    "    targets = torch.stack(targets)\n",
    "    class_sums = torch.sum(targets, dim=(-2, -1))\n",
    "    total_per_class = torch.sum(class_sums, dim=0)\n",
    "    total = torch.sum(total_per_class)\n",
    "    weights = 1 - (total_per_class / total)\n",
    "    torch.save(weights, Path(DATA_DIR / \"class_weights.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformations = T.Compose(\n",
    "    [\n",
    "        T.ToTensor(mask_as_integer=True),\n",
    "        T.Grayscale(),\n",
    "        T.Resize((IMG_DIM, IMG_DIM)),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.RandomVerticalFlip(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transformations = T.Compose(\n",
    "    [\n",
    "        T.ToTensor(mask_as_integer=True),\n",
    "        T.Grayscale(),\n",
    "        T.Resize((IMG_DIM, IMG_DIM)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the experiment we may want to load all annotated images or just the ones that are annotated by experts.\n",
    "\n",
    "Here we create a function to load all the images that are annotated (not only the ones by experts).\n",
    "The created function can than be used to load the data loaders with a specific batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainDataset(\n",
    "    DATA_DIR, transform=train_transformations, with_background=True, as_id_mask=True\n",
    ")\n",
    "val_dataset = ValDataset(\n",
    "    DATA_DIR, transform=val_transformations, with_background=True, as_id_mask=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hubmap.models import FCT\n",
    "from hubmap.models import init_weights\n",
    "\n",
    "model = FCT(in_channels=1, num_classes=4)\n",
    "model = model.apply(init_weights)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.tensor([0.89, 0.05, 0.05, 0.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "# benchmark = IoU()\n",
    "criterion0 = nn.CrossEntropyLoss(weight=weights.to(device))\n",
    "criterion1 = nn.CrossEntropyLoss(weight=weights.to(device))\n",
    "criterion2 = nn.CrossEntropyLoss(weight=weights.to(device))\n",
    "# learning_rate_scheduler = LRScheduler(optimizer, patience=5, min_lr=1e-6, factor=0.8)\n",
    "learning_rate_scheduler = None\n",
    "early_stopping = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = \"fct_224_grayscale.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continue_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 1\n",
    "\n",
    "training_loss_history = []\n",
    "training_metric_history = []\n",
    "\n",
    "validation_loss_history = []\n",
    "validation_metric_history = []\n",
    "\n",
    "if continue_training:\n",
    "    # Load checkpoint.\n",
    "    print(\"Loading checkpoint...\")\n",
    "    checkpoint = torch.load(Path(CHECKPOINT_DIR / checkpoint_name))\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "    training_loss_history = checkpoint[\"training_loss_history\"]\n",
    "    training_metric_history = checkpoint[\"training_metric_history\"]\n",
    "    validation_loss_history = checkpoint[\"validation_loss_history\"]\n",
    "    validation_metric_history = checkpoint[\"validation_metric_history\"]\n",
    "\n",
    "for epoch in tqdm(range(start_epoch, start_epoch + NUM_EPOCHS)):\n",
    "    # tqdm.write(f\"Epoch {epoch}/{num_epochs} - Started training...\")\n",
    "    training_losses = []\n",
    "    training_accuracies = []\n",
    "    model.train()\n",
    "    for images, targets in tqdm(train_loader, leave=False):\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(images)\n",
    "\n",
    "        t1 = (\n",
    "            F.interpolate(targets, size=predictions[0].size()[2:], mode=\"nearest\")\n",
    "            .squeeze(1)\n",
    "            .type(torch.LongTensor)\n",
    "            .to(device)\n",
    "        )\n",
    "        t2 = (\n",
    "            F.interpolate(targets, size=predictions[1].size()[2:], mode=\"nearest\")\n",
    "            .squeeze(1)\n",
    "            .type(torch.LongTensor)\n",
    "            .to(device)\n",
    "        )\n",
    "        t_final = targets.squeeze(1).type(torch.LongTensor).to(device)\n",
    "\n",
    "        # preds_for_loss = F.softmax(preds_for_loss, dim=1)\n",
    "        loss1 = criterion0(predictions[0], t1) * 0.14\n",
    "        loss2 = criterion1(predictions[1], t2) * 0.29\n",
    "        loss_final = criterion2(predictions[2], t_final) * 0.57\n",
    "\n",
    "        loss1.backward(retain_graph=True)\n",
    "        loss2.backward(retain_graph=True)\n",
    "        loss_final.backward(retain_graph=True)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # metric = benchmark(classes_per_channel, targets)\n",
    "        probs = F.softmax(predictions[2], dim=1)\n",
    "        # print(\"probs size: \", probs.size())\n",
    "        classes = torch.argmax(probs, dim=1, keepdim=True)\n",
    "        # print(\"classes size: \", probs.size())\n",
    "\n",
    "        equals = ((classes == 0) == (targets == 0)).sum()\n",
    "        total = (targets == 0).numel()\n",
    "        acc = equals / total\n",
    "        metric = acc\n",
    "\n",
    "        loss = loss1 + loss2 + loss_final\n",
    "        training_losses.append(loss.item())\n",
    "        training_accuracies.append(metric.item())\n",
    "\n",
    "    training_loss_history.append(training_losses)\n",
    "    training_metric_history.append(training_accuracies)\n",
    "\n",
    "    validation_losses = []\n",
    "    validation_accuracies = []\n",
    "    model.eval()\n",
    "    for images, targets in tqdm(val_loader, leave=False):\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = model(images)\n",
    "\n",
    "            t1 = (\n",
    "                F.interpolate(targets, size=predictions[0].size()[2:], mode=\"nearest\")\n",
    "                .squeeze(1)\n",
    "                .type(torch.LongTensor)\n",
    "                .to(device)\n",
    "            )\n",
    "            t2 = (\n",
    "                F.interpolate(targets, size=predictions[1].size()[2:], mode=\"nearest\")\n",
    "                .squeeze(1)\n",
    "                .type(torch.LongTensor)\n",
    "                .to(device)\n",
    "            )\n",
    "            t_final = targets.squeeze(1).type(torch.LongTensor).to(device)\n",
    "\n",
    "            # preds_for_loss = F.softmax(preds_for_loss, dim=1)\n",
    "            loss1 = criterion0(predictions[0], t1) * 0.14\n",
    "            loss2 = criterion1(predictions[1], t2) * 0.29\n",
    "            loss_final = criterion2(predictions[2], t_final) * 0.57\n",
    "\n",
    "            loss = loss1 + loss2 + loss_final\n",
    "\n",
    "            # print(classes.size(), targets.size())\n",
    "            probs = F.softmax(predictions[2], dim=1)\n",
    "            # print(\"probs size: \", probs.size())\n",
    "            classes = torch.argmax(probs, dim=1, keepdim=True)\n",
    "\n",
    "            equals = ((classes == 0) == (targets == 0)).sum()\n",
    "            total = (targets == 0).numel()\n",
    "            acc = equals / total\n",
    "            metric = acc\n",
    "\n",
    "        validation_losses.append(loss.item())\n",
    "        validation_accuracies.append(metric.item())\n",
    "\n",
    "    validation_loss_history.append(validation_losses)\n",
    "    validation_metric_history.append(validation_accuracies)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch}/{NUM_EPOCHS} - Summary: \"\n",
    "        f\"TL: {np.mean(training_losses):.5f} \"\n",
    "        f\"TB: {np.mean(training_accuracies):.5f} \"\n",
    "        f\"VL: {np.mean(validation_losses):.5f} \"\n",
    "        f\"VB: {np.mean(validation_accuracies):.5f}\"\n",
    "    )\n",
    "\n",
    "    data_to_save = {\n",
    "        \"early_stopping\": False,\n",
    "        \"epoch\": epoch,\n",
    "        # \"start_epoch\": start_epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"training_loss_history\": training_loss_history,\n",
    "        \"training_metric_history\": training_metric_history,\n",
    "        \"validation_loss_history\": validation_loss_history,\n",
    "        \"validation_metric_history\": validation_metric_history,\n",
    "    }\n",
    "\n",
    "    # NOW DO THE ADJUSTMENTS USING THE LEARNING RATE SCHEDULER.\n",
    "    if learning_rate_scheduler:\n",
    "        learning_rate_scheduler(np.mean(validation_losses))\n",
    "    # NOW DO THE ADJUSTMENTS USING THE EARLY STOPPING.\n",
    "    if early_stopping:\n",
    "        early_stopping(np.mean(validation_losses))\n",
    "        # MODIFY THE DATA TO SAVE ACCORDING TO THE EARLY STOPPING RESULT.\n",
    "        data_to_save[\"early_stopping\"] = early_stopping.early_stop\n",
    "\n",
    "    # SAVE THE DATA.\n",
    "    torch.save(data_to_save, Path(CHECKPOINT_DIR / checkpoint_name))\n",
    "\n",
    "    # DO THE EARLY STOPPING IF NECESSARY.\n",
    "    if early_stopping and early_stopping.early_stop:\n",
    "        break\n",
    "\n",
    "result = {\n",
    "    \"epoch\": epoch,\n",
    "    \"training\": {\n",
    "        \"loss\": training_loss_history,\n",
    "        \"metric\": training_metric_history,\n",
    "    },\n",
    "    \"validation\": {\n",
    "        \"loss\": validation_loss_history,\n",
    "        \"metric\": validation_metric_history,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visualize the results.<br>\n",
    "(*this needs improvements + better and more visualizations for the final paper*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hubmap.visualization import visualize_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fig, benchmark_fig = visualize_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "J",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a58ea3f06b7c035f03696c8d262430e329e4f861a31188ab1909a2b4ddd27d3d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
